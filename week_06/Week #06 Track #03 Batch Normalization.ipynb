{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Week #06 Track #03 Batch Normalization.ipynb","provenance":[],"collapsed_sections":[],"toc_visible":true,"authorship_tag":"ABX9TyNGK9TN93fpHnMO0nkGhzBz"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"r6LB4RyNr0pn"},"source":["# 1 - Accelerate Learning with Batch Normalization"]},{"cell_type":"markdown","metadata":{"id":"MwJEaq7SsJYQ"},"source":["**Training deep neural networks** with tens of layers is challenging as they can be **sensitive to the initial random weights** and configuration of the learning algorithm. \n","\n","One possible reason for this difficulty is: \n","\n","> the distribution of the inputs to layers deep in the network may change after\n","each minibatch when the weights are updated. \n","\n","This can cause the learning algorithm to chase a moving target forever. This change in the distribution of inputs to layers in the network is referred to by the technical name **internal covariate shift**. \n","\n","**Batch normalization** is a technique for training very deep neural networks that standardize each minibatch layer's inputs. This has the effect of stabilizing the learning process and dramatically reducing the number of training epochs required to train deep networks. This section will discover the batch normalization method used to accelerate deep learning neural networks training. After\n","reading this section, you will know:\n","\n","- Deep neural networks are challenging to train, not least because the input from prior layers can change after weight updates.\n","\n","- Batch normalization is a technique to standardize the inputs to a network, applied to either the activations of a prior layer or inputs directly.\n","\n","- Batch normalization accelerates training, in some cases by halving the number of epochs (or better), and provides some regularization effect, reducing generalization error."]},{"cell_type":"markdown","metadata":{"id":"OH9mCk8fvRAH"},"source":["## 1.1 Batch Normalization"]},{"cell_type":"markdown","metadata":{"id":"m0DvdGy5EH1d"},"source":["Training deep neural networks, e.g., networks with tens of hidden layers, is challenging. One aspect of this challenge is that the model is updated layer-by-layer backward from the output to the input using an **estimate of error that assumes the weights in the layers prior to the current\n","the layer are fixed**.\n","\n","> Very deep models involve the composition of several functions or layers. The gradient tells how to update each parameter, under the assumption that the other layers do not change. In practice, we update all of the layers simultaneously.\n","\n","**Because all layers are changed during an update**, the update procedure is forever chasing a moving target. For example, the weights of a layer are updated given an expectation that the prior layer outputs values with a given distribution. This distribution is likely changed after the\n","weights of the prior layer are updated.\n","\n","\n","> Training Deep Neural Networks is complicated by the fact that **the distribution of each layer's inputs changes during training as the parameters of the previous layers changes**. This slows down the training by **requiring lower learning rates** and **careful parameter initialization**, making it notoriously hard to train models with saturating nonlinearities."]},{"cell_type":"markdown","metadata":{"id":"YUR6FVtmE0My"},"source":["## 1.2 Standardize Layer Inputs"]},{"cell_type":"markdown","metadata":{"id":"6pCPWZmKF0yV"},"source":["Batch normalization, or **batch norm** for short, is [proposed as a technique](https://arxiv.org/pdf/1502.03167.pdf) to help coordinate the update of multiple layers in the model.\n","\n","> Batch normalization provides an elegant way of reparametrizing almost any deep network. The reparametrization significantly **reduces the problem of coordinating updates across many layers**.\n","\n","It does this by scaling the layer's output, specifically by **standardizing the activations of each input variable per minibatch**, such as the activations of a node from the previous layer. Recall that standardization refers to rescaling data to have a **mean of zero** and a **standard deviation of one**, e.g., a standard Gaussian.\n","\n","Standardizing the activations of the prior layer means that assumptions the subsequent layer **makes about the spread and distribution of inputs during the weight update will not change**, at least not dramatically. <font color=\"red\">This has the effect of stabilizing and speeding-up the training process of deep neural networks</font>.\n","\n","> Batch normalization acts to standardize only the mean and variance of each unit in order to stabilize learning but allows the relationships between units and the nonlinear statistics of a single unit to change.\n","\n","Normalizing the inputs to the layer affects the model's training, dramatically reducing the number of epochs required. **It can also have a regularizing effect**, reducing generalization error much like the use of activation regularization.\n","\n","Although **reducing internal covariate shift** was a motivation in the development of the method,\n","there is some suggestion that instead batch normalization is effective because it smooths and, in\n","turn, **simplifies the optimization function that is being solved when training the network**.\n","\n","> According to a [recent paper](https://arxiv.org/pdf/1805.11604.pdf), BatchNorm impacts network training fundamentally: **it makes the landscape of the corresponding optimization problem be significantly more smooth**. This ensures, in particular, that <font color=\"red\">the gradients are more predictive and thus allow for the use of a more extensive range of learning rates and faster network convergence</font>."]},{"cell_type":"markdown","metadata":{"id":"I-o0htyLGIZT"},"source":["## 1.3 How to Standardize Layer Inputs"]},{"cell_type":"markdown","metadata":{"id":"KwnLPVyc9mLL"},"source":["Batch normalization can be **implemented during training by calculating each input variable's mean and standard deviation to a layer per minibatch** and using these statistics to perform the standardization. Alternately, a running average of mean and standard deviation can be\n","maintained across mini-batches but may result in unstable training.\n","\n","This standardization of inputs may be applied to input variables for the first hidden layer or the activations from a hidden layer for deeper layers. In practice, it is common to allow the layer to learn two new parameters, namely a new mean and standard deviation, **Beta** and\n","**Gamma** respectively, that allow the automatic scaling and shifting of the standardized layer inputs. The model learns these parameters as part of the training process.\n","\n","> Note that simply normalizing each input of a layer may change what the layer can represent. **These parameters are learned along with the original model parameters and restore the network's representation power**.\n","\n","Significantly the backpropagation algorithm is updated to operate upon the transformed inputs, and error is also used to update the new scale and shifting parameters learned by the model. The standardization is applied to the inputs to the layer, namely the input variables or the output of the activation function from the last layer. Given the choice of activation function, the input distribution to the layer may be pretty non-Gaussian. In this case, there may be a benefit in standardizing the summed activation before the activation function in the previous layer.\n","\n","\n","> **We add the BN transform immediately before the nonlinearity**. We could have also normalized the layer inputs *u*, but since *u* is likely the output of another nonlinearity, the shape of its distribution is likely to change during training, and constraining its first and second moments would not eliminate the covariate shift."]},{"cell_type":"markdown","metadata":{"id":"s_hUcvfK_1Fc"},"source":["## 1.4 Tips for Using Batch Normalization"]},{"cell_type":"markdown","metadata":{"id":"7_bQaaBhLA_0"},"source":["This section provides tips and suggestions for using batch normalization with your own neural networks.\n","\n","**Use With Different Network Types**\n","\n","> Batch normalization is a general technique that can be used to normalize the inputs to a layer. It can be used with most network types, such as **Multilayer Perceptrons**, **Convolutional Neural Networks**, and **Recurrent Neural Networks**.\n","\n","\n","**Probably Use Before the Activation**\n","\n","> Batch normalization may be used on the inputs to the layer before or after the activation function in the previous layer. It may be more **appropriate after the activation function for s-shaped functions** like the hyperbolic tangent and logistic function. It may be appropriate **before the activation function** for activations that may result in non-Gaussian distributions like\n","the **rectified linear activation function**, the modern default for most network types.\n","\n","The goal of Batch Normalization is to achieve a stable distribution of activation values throughout training. In experiments conducted in the [original paper]((https://arxiv.org/pdf/1502.03167.pdf)), authors applied it before the nonlinearity since matching the first and second moments is more likely to result in a stable distribution.\n","\n","**Use Large Learning Rates**\n","\n","> Using batch normalization makes the network more stable during training. This may require a much greater learning rate than standard learning rates, which may further speed up the learning process.\n","\n","**Less Sensitive to Weight Initialization**\n","\n","> Deep neural networks can be pretty sensitive to the technique used to initialize the weights before training. The stability to training brought by batch normalization can make training deep networks less sensitive to the weight initialization method's choice.\n","\n","**Do not Use With Dropout**\n","\n","> Batch normalization offers some regularization effect, reducing generalization error, perhaps no longer requiring dropout for regularization.\n","\n","Further, it may not be good to use batch normalization and dropout in the same network. The reason is that the statistics used to normalize the prior layer's activations may become noisy given the random dropping out of nodes during the dropout procedure."]},{"cell_type":"markdown","metadata":{"id":"WPSCaslMNf-1"},"source":["## 1.5 Batch Normalization Case Study"]},{"cell_type":"code","metadata":{"id":"Hwvh-fPgTd7Q"},"source":["# scatter plot of the circles dataset with points colored by class\n","from sklearn.datasets import make_circles\n","import numpy as np\n","import matplotlib.pyplot as plt\n","\n","# generate circles\n","x, y = make_circles(n_samples=1000, noise=0.1, random_state=1)\n","\n","# select indices of points with each class label\n","for i in range(2):\n","\tsamples_ix = np.where(y == i)\n","\tplt.scatter(x[samples_ix, 0], x[samples_ix, 1], label=str(i))\n","plt.legend()\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"D0ZGE8hHUfBd"},"source":["### 1.5.1 Multilayer Perceptron Model"]},{"cell_type":"code","metadata":{"id":"HPNBZYQGzjeq"},"source":["%%capture\n","!pip install wandb==0.10.17"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"mhJxFQyFzmea"},"source":["!wandb login"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"zfmevaH6UNns"},"source":["# mlp for the two circles problem\n","from sklearn.datasets import make_circles\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Dense\n","from tensorflow.keras.optimizers import SGD\n","import matplotlib.pyplot as plt\n","import wandb\n","from wandb.keras import WandbCallback"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"xrS0KFjJgQxl"},"source":["# Default values for hyperparameters\n","defaults = dict(layer_1 = 50,\n","                learn_rate = 0.01,\n","                batch_size = 32,\n","                epoch = 100)\n","\n","wandb.init(project=\"week06_bn\", \n","           config= defaults, \n","           name=\"week06_bn_run_01\")\n","config = wandb.config"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"QWWarzqNgPDf"},"source":["%%wandb\n","\n","# generate 2d classification dataset\n","x, y = make_circles(n_samples=1000, noise=0.1, random_state=1)\n","\n","# split into train and test\n","n_train = 500\n","train_x, test_x = x[:n_train, :], x[n_train:, :]\n","train_y, test_y = y[:n_train], y[n_train:]\n","\n","# define model\n","model = Sequential()\n","model.add(Dense(config.layer_1, input_dim=2, \n","                activation='relu', \n","                kernel_initializer='he_uniform'))\n","model.add(Dense(1, activation='sigmoid'))\n","opt = SGD(learning_rate=config.learn_rate, momentum=0.9)\n","model.compile(loss='binary_crossentropy', \n","              optimizer=opt, metrics=['accuracy'])\n","\n","# fit model\n","history = model.fit(train_x, train_y, \n","                    validation_data=(test_x, test_y), \n","                    epochs=config.epoch, verbose=0, \n","                    batch_size=config.batch_size,\n","                    callbacks=[WandbCallback(log_weights=True,\n","                                             log_gradients=True,\n","                                             training_data=(train_x,train_y))])\n","\n","# for more elaborate results please see the project in wandb"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"BM68FYWygrql"},"source":["# evaluate the model\n","_, train_acc = model.evaluate(train_x, train_y, verbose=0)\n","_, test_acc = model.evaluate(test_x, test_y, verbose=0)\n","print('Train: %.3f, Test: %.3f' % (train_acc, test_acc))\n","\n","# plot loss learning curves\n","plt.subplot(211)\n","plt.title('Cross-Entropy Loss', pad=-40)\n","plt.plot(history.history['loss'], label='train')\n","plt.plot(history.history['val_loss'], label='test')\n","plt.legend()\n","# plot accuracy learning curves\n","plt.subplot(212)\n","plt.title('Accuracy', pad=-40)\n","plt.plot(history.history['accuracy'], label='train')\n","plt.plot(history.history['val_accuracy'], label='test')\n","plt.legend()\n","plt.tight_layout()\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"574ahaQXcmfs"},"source":["model.summary()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Pgf9llT9VF2P"},"source":["### 1.5.2 Multilayer Perceptron with Batch Normalization"]},{"cell_type":"markdown","metadata":{"id":"bpQNrJNZWipY"},"source":["The model introduced in the previous section can be updated to add batch normalization. The expectation is that batch normalization would accelerate the training process, offering similar or better classification accuracy in fewer training epochs. Batch normalization is also reported as providing a subtle form of regularization, meaning that it may also offer a slight reduction in generalization error demonstrated by a small increase in classification accuracy on the holdout test dataset. A new BatchNormalization layer can be added to the model after the hidden layer before the output layer. Specifically, after the activation function of the last hidden layer."]},{"cell_type":"code","metadata":{"id":"tuKBhad8W6yv"},"source":["# mlp for the two circles problem with batchnorm after activation function\n","from sklearn.datasets import make_circles\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Dense\n","from tensorflow.keras.layers import BatchNormalization\n","from tensorflow.keras.optimizers import SGD\n","import matplotlib.pyplot as plt\n","import wandb\n","from wandb.keras import WandbCallback"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"4PMvo2EmhZhH"},"source":["# Default values for hyperparameters\n","defaults = dict(layer_1 = 50,\n","                learn_rate = 0.01,\n","                batch_size = 32,\n","                epoch = 100)\n","\n","wandb.init(project=\"week06_bn\", config= defaults, name=\"week06_bn_run_02\")\n","config = wandb.config"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"NHw-g-THhQWv"},"source":["%%wandb\n","\n","# mlp for the two circles problem with batchnorm after activation function\n","from sklearn.datasets import make_circles\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Dense\n","from tensorflow.keras.layers import BatchNormalization\n","from tensorflow.keras.optimizers import SGD\n","import matplotlib.pyplot as plt\n","\n","# generate 2d classification dataset\n","x, y = make_circles(n_samples=1000, noise=0.1, random_state=1)\n","\n","# split into train and test\n","n_train = 500\n","train_x, test_x = x[:n_train, :], x[n_train:, :]\n","train_y, test_y = y[:n_train], y[n_train:]\n","\n","# define model\n","model = Sequential()\n","model.add(Dense(config.layer_1, \n","                input_dim=2, \n","                activation='relu', kernel_initializer='he_uniform'))\n","model.add(BatchNormalization())\n","model.add(Dense(1, activation='sigmoid'))\n","opt = SGD(learning_rate=config.learn_rate, momentum=0.9)\n","model.compile(loss='binary_crossentropy', optimizer=opt, metrics=['accuracy'])\n","\n","# fit model\n","history = model.fit(train_x, train_y,\n","                    validation_data=(test_x, test_y), \n","                    epochs=config.epoch, verbose=0,\n","                    batch_size=config.batch_size,\n","                    callbacks=[WandbCallback(log_weights=True,\n","                                             log_gradients=True,\n","                                             training_data=(train_x,train_y))])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"btAlfnKkhoqM"},"source":["# evaluate the model\n","_, train_acc = model.evaluate(train_x, train_y, verbose=0)\n","_, test_acc = model.evaluate(test_x, test_y, verbose=0)\n","print('Train: %.3f, Test: %.3f' % (train_acc, test_acc))\n","\n","# plot loss learning curves\n","plt.subplot(211)\n","plt.title('Cross-Entropy Loss', pad=-40)\n","plt.plot(history.history['loss'], label='train')\n","plt.plot(history.history['val_loss'], label='test')\n","plt.legend()\n","# plot accuracy learning curves\n","plt.subplot(212)\n","plt.title('Accuracy', pad=-40)\n","plt.plot(history.history['accuracy'], label='train')\n","plt.plot(history.history['val_accuracy'], label='test')\n","plt.legend()\n","plt.tight_layout()\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"rPOC2Z-uc6tU"},"source":["# tensorflow.kera use non-trainable params with batch normalization\n","# in order to maintain auxiliary variables used in inference\n","model.summary()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"oksQ49r9XsZT"},"source":["In this case, we can see the model's comparable performance on both the train and test set of about 84% accuracy, very similar to what we saw in the previous section, if not a little bit better.\n","\n","A graph of the learning curves is also created, showing classification accuracy on each training epoch's train and test sets. In this case, we can see that the model has learned the problem faster than the model in the previous section without batch normalization. Specifically,\n","**we can see that classification accuracy on the train and test datasets leap above 80% within the first 20 epochs instead of 30-to-40 epochs in the model without batch normalization**. The plot also shows the effect of batch normalization during training. We can see lower performance\n","on the training dataset than the test dataset: scores on the training dataset that are lower than the test dataset's performance at the end of the training run. This is likely the effect of the input collected and updated each minibatch.\n"]},{"cell_type":"markdown","metadata":{"id":"eDa2D7dCY6f8"},"source":["We can also try a variation of the model where batch normalization is applied prior to the activation function of the hidden layer, instead of after the activation function."]},{"cell_type":"code","metadata":{"id":"Hcawb4h2ZJy7"},"source":["# mlp for the two circles problem with batchnorm before activation function\n","from sklearn.datasets import make_circles\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Dense\n","from tensorflow.keras.layers import Activation\n","from tensorflow.keras.layers import BatchNormalization\n","from tensorflow.keras.optimizers import SGD\n","import matplotlib.pyplot as plt"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"edqyvj9cipVV"},"source":["# Default values for hyperparameters\n","defaults = dict(layer_1 = 50,\n","                learn_rate = 0.01,\n","                batch_size = 32,\n","                epoch = 100)\n","\n","wandb.init(project=\"week06_bn\", config= defaults, name=\"week06_bn_run_03\")\n","config = wandb.config"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"MUtckY_8iJzq"},"source":["%%wandb\n","# generate 2d classification dataset\n","x, y = make_circles(n_samples=1000, noise=0.1, random_state=1)\n","\n","# split into train and test\n","n_train = 500\n","train_x, test_x = x[:n_train, :], x[n_train:, :]\n","train_y, test_y = y[:n_train], y[n_train:]\n","\n","# define model\n","model = Sequential()\n","model.add(Dense(config.layer_1, input_dim=2, kernel_initializer='he_uniform'))\n","model.add(BatchNormalization())\n","model.add(Activation('relu'))\n","model.add(Dense(1, activation='sigmoid'))\n","opt = SGD(learning_rate=config.learn_rate, momentum=0.9)\n","model.compile(loss='binary_crossentropy', optimizer=opt, metrics=['accuracy'])\n","\n","# fit model\n","history = model.fit(train_x, train_y, \n","                    validation_data=(test_x, test_y), \n","                    epochs=config.epoch, verbose=0,\n","                    callbacks=[WandbCallback(log_weights=True,\n","                                             log_gradients=True,\n","                                             training_data=(train_x,train_y))])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"kIaCk10iTnVX"},"source":["# tensorflow.kera use non-trainable params with batch normalization\n","# in order to maintain auxiliary variables used in inference\n","model.summary()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"rFTRwvw9iXXs"},"source":["# evaluate the model\n","_, train_acc = model.evaluate(train_x, train_y, verbose=0)\n","_, test_acc = model.evaluate(test_x, test_y, verbose=0)\n","print('Train: %.3f, Test: %.3f' % (train_acc, test_acc))\n","\n","# plot loss learning curves\n","plt.subplot(211)\n","plt.title('Cross-Entropy Loss', pad=-40)\n","plt.plot(history.history['loss'], label='train')\n","plt.plot(history.history['val_loss'], label='test')\n","plt.legend()\n","# plot accuracy learning curves\n","plt.subplot(212)\n","plt.title('Accuracy', pad=-40)\n","plt.plot(history.history['accuracy'], label='train')\n","plt.plot(history.history['val_accuracy'], label='test')\n","plt.legend()\n","plt.tight_layout()\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"eP4yiqS9Zqe0"},"source":["In this case, we can see the model's comparable performance on the train and test datasets, but slightly worse than the model without batch normalization.\n","\n","The line plot of the learning curves on the train and test sets also tells a different story. The plot shows the model learning perhaps at the same pace as the model without batch normalization, but the model's performance on the training dataset is much worse, hovering around 70% to 75% accuracy, again likely an effect of the statistics collected and used over each minibatch. At least for this model configuration on this specific dataset, it appears that batch normalization is more effective after the rectified linear activation function."]},{"cell_type":"markdown","metadata":{"id":"H8arWJv1ajh7"},"source":["### 1.5.3 Extensions"]},{"cell_type":"markdown","metadata":{"id":"5ABzuop7atwz"},"source":["This section lists some ideas for extending the case study that you may wish to explore.\n","\n","- **Without Beta and Gamma**: update the example to not use the beta and gamma parameters in the batch normalization layer and compare results.\n","- **Without Momentum**: update the example not to use momentum in the batch normalization layer during training and compare results.\n","- **Input Layer**: update the example to use batch normalization after the input to the model and compare results."]}]}