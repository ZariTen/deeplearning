{"nbformat":4,"nbformat_minor":0,"metadata":{"accelerator":"GPU","colab":{"name":"Week #06 Task #01 Hyperparameter Tuning using Keras Tuner.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"display_name":"Python 3","name":"python3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"7DLkQmFwhbq4"},"source":["# A brief recap about DL Pipeline"]},{"cell_type":"markdown","metadata":{"id":"VH2FWhOPNzZO"},"source":["- Define the task\n","  - Frame the problem\n","  - Collect a dataset\n","  - Understand your data\n","  - Choose a measure of success\n","- Develop a model\n","  - Prepare the data\n","  - Choose an evaluation protocol\n","  - Beat a baseline\n","  - Scale up: develop a model that overfits\n","  - Regularize and tune your model\n","- Deploy your model\n","  - Explain your work to stakeholders and set expectations\n","    - Ship an inference model\n","    - Deploying a model as a rest API\n","    - Deploying a model on device\n","    - Deploying a model in the browser\n","  - Monitor your model in the wild\n","  - Maintain your model\n"]},{"cell_type":"markdown","metadata":{"id":"cTShha8tLAvY"},"source":["# 1.0 Baseline Model"]},{"cell_type":"markdown","metadata":{"id":"RD1hMKLZEObd"},"source":["## 1.1 Import Libraries"]},{"cell_type":"markdown","metadata":{"id":"4MR6rLenKnry"},"source":["Install and import the Keras Tuner."]},{"cell_type":"code","metadata":{"id":"rEYDnz5LKra8"},"source":["# pip install -q (quiet)\n","!pip install git+https://github.com/keras-team/keras-tuner.git -q"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"jtGJ6_Pi2yG9"},"source":["import tensorflow as tf\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import h5py\n","import time\n","import datetime\n","import pytz\n","import IPython\n","import keras_tuner as kt"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ccfIzxFaHeOb"},"source":["print('TF version:', tf.__version__)\n","print('KT version:', kt.__version__)\n","print('GPU devices:', tf.config.list_physical_devices('GPU'))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"0WGGeZQGESGP"},"source":["## 1.2 Utils Functions"]},{"cell_type":"code","metadata":{"id":"hw9z0s-QqSCd"},"source":["# download train_catvnoncat.h5\n","!gdown https://drive.google.com/uc?id=1ZPWKlEATuDjFtZJPgHCc5SURrcKaVP9Z\n","\n","# download test_catvnoncat.h5\n","!gdown https://drive.google.com/uc?id=1ndRNAwidOqEgqDHBurA0PGyXqHBlvzz-"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"wJYkR70dEGHh"},"source":["def load_dataset():\n","    # load the train data\n","    train_dataset = h5py.File('train_catvnoncat.h5', \"r\")\n","\n","    # your train set features\n","    train_set_x_orig = np.array(train_dataset[\"train_set_x\"][:]) \n","\n","    # your train set labels\n","    train_set_y_orig = np.array(train_dataset[\"train_set_y\"][:]) \n","\n","    # load the test data\n","    test_dataset = h5py.File('test_catvnoncat.h5', \"r\")\n","\n","    # your test set features\n","    test_set_x_orig = np.array(test_dataset[\"test_set_x\"][:]) \n","\n","    # your test set labels  \n","    test_set_y_orig = np.array(test_dataset[\"test_set_y\"][:]) \n","\n","    # the list of classes\n","    classes = np.array(test_dataset[\"list_classes\"][:]) \n","\n","    # reshape the test data\n","    train_set_y_orig = train_set_y_orig.reshape((train_set_y_orig.shape[0],1))\n","    test_set_y_orig = test_set_y_orig.reshape((test_set_y_orig.shape[0],1))\n","\n","    return train_set_x_orig, train_set_y_orig, test_set_x_orig, test_set_y_orig, classes"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"exIeT2zMHhxa"},"source":["## 1.3 Load Dataset"]},{"cell_type":"code","metadata":{"id":"mjQYsrdPHSyh"},"source":["# Loading the data (cat/non-cat)\n","train_set_x_orig, train_y, test_set_x_orig, test_y, classes = load_dataset()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"6ihL83MbHlhc"},"source":["# Reshape the training and test examples\n","train_set_x_flatten = train_set_x_orig.reshape(train_set_x_orig.shape[0],-1)\n","test_set_x_flatten = test_set_x_orig.reshape(test_set_x_orig.shape[0],-1)\n","\n","# Standardize the dataset\n","train_x = train_set_x_flatten/255\n","test_x = test_set_x_flatten/255"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"sQn-Brt-IhK_"},"source":["print (\"train_x shape: \" + str(train_x.shape))\n","print (\"train_y shape: \" + str(train_y.shape))\n","print (\"test_x  shape: \" + str(test_x.shape))\n","print (\"test_y  shape: \" + str(test_y.shape))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Xa9dcSRx1__x"},"source":["# visualize a sample data\n","index = 13\n","plt.imshow(train_set_x_orig[index])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"5zLrb3ORIlo_"},"source":["## 1.4 Model"]},{"cell_type":"code","metadata":{"id":"zJutkmXnIpG6"},"source":["class MyCustomCallback(tf.keras.callbacks.Callback):\n","\n","  def on_train_begin(self, batch, logs=None):\n","    self.begins = time.time()\n","    print('Training: begins at {}'.format(datetime.datetime.now(pytz.timezone('America/Fortaleza')).strftime(\"%a, %d %b %Y %H:%M:%S\")))\n","\n","  def on_train_end(self, logs=None):\n","    print('Training: ends at {}'.format(datetime.datetime.now(pytz.timezone('America/Fortaleza')).strftime(\"%a, %d %b %Y %H:%M:%S\")))\n","    print('Duration: {:.2f} seconds'.format(time.time() - self.begins))    "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"SyfcUdH36lGG"},"source":["# Instantiate a simple classification model\n","model = tf.keras.Sequential([\n","  tf.keras.layers.Dense(8, activation=tf.nn.relu, dtype='float64'),\n","  tf.keras.layers.Dense(8, activation=tf.nn.relu, dtype='float64'),\n","  tf.keras.layers.Dense(1, activation=tf.nn.sigmoid, dtype='float64')\n","])\n","\n","# Instantiate a logistic loss function that expects integer targets.\n","loss = tf.keras.losses.BinaryCrossentropy()\n","\n","# Instantiate an accuracy metric.\n","accuracy = tf.keras.metrics.BinaryAccuracy()\n","\n","# Instantiate an optimizer.\n","optimizer = tf.keras.optimizers.SGD(learning_rate=0.001)\n","\n","# configure the optimizer, loss, and metrics to monitor.\n","model.compile(optimizer=optimizer, loss=loss, metrics=[accuracy])\n","\n","# training \n","history = model.fit(x=train_x,\n","                    y=train_y,\n","                    batch_size=32,\n","                    epochs=500,\n","                    validation_data=(test_x,test_y),\n","                    callbacks=[MyCustomCallback()],\n","                    verbose=1)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"fugflUT5JtCe"},"source":["loss, acc = model.evaluate(x=train_x,y=train_y, batch_size=32)\n","print('Train loss: %.4f - acc: %.4f' % (loss, acc))\n","\n","loss_, acc_ = model.evaluate(x=test_x,y=test_y, batch_size=32)\n","print('Test loss: %.4f - acc: %.4f' % (loss_, acc_))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"q6CsWRrqJzFQ"},"source":["# 2.0 Hyperparameter Tuning using Keras-Tuner"]},{"cell_type":"markdown","metadata":{"id":"yff1zTsrLJ4J"},"source":["The [Keras Tuner](https://github.com/keras-team/keras-tuner) is a library that helps you pick the optimal set of hyperparameters for your TensorFlow program. The process of selecting the right set of hyperparameters for your machine learning (ML) application is called **hyperparameter tuning** or **hypertuning**. \n","\n","Hyperparameters are the variables that govern the training process and the topology of an ML model. These variables remain constant over the training process and directly impact the performance of your ML program. Hyperparameters are of two types:\n","1. **Model hyperparameters** which influence model selection such as the number and width of hidden layers\n","2. **Algorithm hyperparameters** which influence the speed and quality of the learning algorithm such as the learning rate for Stochastic Gradient Descent (SGD) and the number of nearest neighbors for a k Nearest Neighbors (KNN) classifier, among others.\n"]},{"cell_type":"markdown","metadata":{"id":"K5YEL2H2Ax3e"},"source":["## 2.1 Define the model\n"]},{"cell_type":"markdown","metadata":{"id":"md7FhKoYcuj7"},"source":["\n","When you build a model for hypertuning, you also define the hyperparameter search space in addition to the model architecture. The model you set up for hypertuning is called a **hypermodel**.\n","\n","You can define a hypermodel through two approaches:\n","\n","* By using a model builder function\n","* By subclassing the `HyperModel` class of the Keras Tuner API\n","\n","You can also use two pre-defined `HyperModel` classes - [HyperXception](https://keras-team.github.io/keras-tuner/documentation/hypermodels/#hyperxception-class) and [HyperResNet](https://keras-team.github.io/keras-tuner/documentation/hypermodels/#hyperresnet-class) for computer vision applications.\n","\n","In this section, you use a model builder function to define the image classification model. The model builder function returns a compiled model and uses hyperparameters you define inline to hypertune the model."]},{"cell_type":"code","metadata":{"id":"M3Iosz7dctGf"},"source":["def model_builder(hp):\n","  # Instantiate a simple classification model\n","  model = tf.keras.Sequential()\n","\n","  # Tune the number of units in the first Dense layer\n","  # Choose an optimal value between 8-32\n","  hp_units = hp.Int('units', min_value = 8, max_value = 32, step = 8)\n","  model.add(tf.keras.layers.Dense(hp_units, activation=tf.nn.relu, dtype='float64'))\n","  model.add(tf.keras.layers.Dense(8, activation=tf.nn.relu, dtype='float64'))\n","  model.add(tf.keras.layers.Dense(1, activation=tf.nn.sigmoid, dtype='float64'))\n","\n","  # Instantiate a logistic loss function that expects integer targets.\n","  loss = tf.keras.losses.BinaryCrossentropy()\n","\n","  # Instantiate an accuracy metric.\n","  accuracy = tf.keras.metrics.BinaryAccuracy()\n","\n","  # Instantiate an optimizer.\n","  optimizer = tf.keras.optimizers.SGD(learning_rate=0.001)\n","\n","  # configure the optimizer, loss, and metrics to monitor.\n","  model.compile(optimizer=optimizer, loss=loss, metrics=[accuracy])\n","\n","  return model"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"L8bYqKpDeBlB"},"source":["## 2.2 Instantiate the tuner and perform hypertuning\n"]},{"cell_type":"markdown","metadata":{"id":"pYX1lhvDeDoz"},"source":["\n","Instantiate the tuner to perform the hypertuning. The Keras Tuner has [four tuners available](https://keras-team.github.io/keras-tuner/documentation/tuners/) - `RandomSearch`, `Hyperband`, `BayesianOptimization`, and `Sklearn`. \n","\n","Notice that in previous subsection we're not fitting there, and we're returning the compiled model. Let's continue to build out the rest of our program first, then we'll make things more dynamic. Adding the dynamic bits will all happen in the **model_builder** function, but we will need some other code that will use this function now.  To start, we're going to import **RandomSearch** and after that we'll first define our tuner."]},{"cell_type":"code","metadata":{"id":"BOzyY-KLhQd5"},"source":["from keras_tuner.tuners import RandomSearch"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"V4JXxAYAe67h"},"source":["# path to store results\n","LOG_DIR = f\"{int(time.time())}\""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Ikjt5YbnepK5"},"source":["tuner = RandomSearch(model_builder,\n","                     objective='val_binary_accuracy',\n","                     max_trials=4,  # how many model configurations would you like to test?\n","                     executions_per_trial=1,  # how many trials per variation? (same model could perform differently)\n","                     directory=LOG_DIR,\n","                     project_name=\"my_first_tuner\"\n","                     )"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"d8LgEGSAhewb"},"source":["- Your objective here probably should be **validation accuracy**, but you can choose from other things like **val_loss** for example.\n","- **max_trials** allows you limit how many tests will be run. If you put 10 here, you will get 10 different tests (provided you've specified enough variability for 10 different combinations, anyway).\n","- **executions_per_trial** might be 1, but you might also do many more like 3,5, or even 10.\n","\n","Basically, if you're just hunting for a model that works, then you should just do 1 trial per variation. If you're attempting to seek out 1-3% on **validation accuracy**, then you should run 3+ trials most likely per model, because each time a model runs, you should see some variation in final values. So this will just depend on what kind of a search you're doing (just trying to find something that works vs fine tuning...or anything in between)."]},{"cell_type":"markdown","metadata":{"id":"fJwb5NZgiWXt"},"source":["Run the hyperparameter search. The arguments for the search method are the same as those used for [`tf.keras.model.fit`](https://www.tensorflow.org/api_docs/python/tf/keras/Model#fit).\n","\n","Before running the hyperparameter search, define a callback to clear the training outputs at the end of every training step."]},{"cell_type":"code","metadata":{"id":"nK6XNozXlJK7"},"source":["class ClearTrainingOutput(tf.keras.callbacks.Callback):\n","  def on_train_end(*args, **kwargs):\n","    IPython.display.clear_output(wait = True)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"PyNf7zRakAcr"},"source":["tuner.search(train_x,\n","             train_y, \n","             epochs = 500, \n","             verbose=1,\n","             batch_size=32,\n","             validation_data = (test_x, test_y),\n","             callbacks = [ClearTrainingOutput()])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Th3JZnM5kX_k"},"source":["# print a summary of results\n","tuner.results_summary(num_trials=10)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ICBtBvvSm7AE"},"source":["# best hyperparameters is a dictionary\n","tuner.get_best_hyperparameters()[0].values"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"_O9cyI_kK0US"},"source":["# search space summary\n","tuner.search_space_summary()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"xfJ3aiJ5p9iv"},"source":["print(f\"\"\"The hyperparameter search is complete. The optimal number of units in the first densely-connected\n","layer is {tuner.get_best_hyperparameters()[0].values.get('units')}\"\"\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"uvyvW9oZwkgJ"},"source":["## 2.3 Playing with search space"]},{"cell_type":"markdown","metadata":{"id":"-cIpO1GkxTgZ"},"source":["We also can play with the search space in order to contain conditional hyperparameters. Below, we have a **for loop** creating a **tunable number of layers**, which themselves involve a tunable **units** parameter. This can be pushed to any level of parameter interdependency, including recursion. Note that all parameter names should be unique (here, in the loop over **i**, we name the inner parameters **'units_'** + **str(i)**)."]},{"cell_type":"code","metadata":{"id":"5ZPJ9lSLyEfg"},"source":["def model_builder_all(hp):\n","  # Instantiate a simple classification model\n","  model = tf.keras.Sequential()\n","  \n","  # Create a tunable number of layers 1,2,3,4\n","  for i in range(hp.Int('num_layers', 1, 4)):\n","\n","    # Tune the number of units in the Dense layer\n","    # Choose an optimal value between 8-32\n","    model.add(tf.keras.layers.Dense(units=hp.Int('units_' + str(i),\n","                                        min_value = 8,\n","                                        max_value = 32,\n","                                        step = 8),\n","                                    # Tune the activation functions\n","                                    activation= hp.Choice('dense_activation_' + str(i),\n","                                                          values=['relu', 'tanh'],\n","                                                          default='relu'),\n","                                    dtype='float64'))\n","\n","  model.add(tf.keras.layers.Dense(1, activation=tf.nn.sigmoid, dtype='float64'))\n","\n","  # Instantiate a logistic loss function that expects integer targets.\n","  loss = tf.keras.losses.BinaryCrossentropy()\n","\n","  # Instantiate an accuracy metric.\n","  accuracy = tf.keras.metrics.BinaryAccuracy()\n","\n","  optimizer = hp.Choice('optimizer', ['adam', 'SGD'])\n","  if optimizer == 'adam':\n","    opt = tf.keras.optimizers.Adam(learning_rate=hp.Float('lrate_adam',\n","                                                          min_value=1e-4,\n","                                                          max_value=1e-2, \n","                                                          sampling='LOG'))\n","  else:\n","    opt = tf.keras.optimizers.SGD(learning_rate=hp.Float('lrate_sgd',\n","                                                          min_value=1e-4,\n","                                                          max_value=1e-2, \n","                                                          sampling='LOG'))\n","\n","  # configure the optimizer, loss, and metrics to monitor.\n","  model.compile(optimizer=opt, loss=loss, metrics=[accuracy])\n","\n","  return model"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"3GuC7Drk2ZLC"},"source":["# path to store results\n","LOG_DIR = f\"{int(time.time())}\""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Xc7BSy102ZLI"},"source":["tuner_ = RandomSearch(model_builder_all,\n","                     objective='val_binary_accuracy',\n","                     max_trials=20,  # how many model configurations would you like to test?\n","                     executions_per_trial=1,  # how many trials per variation? (same model could perform differently)\n","                     directory=LOG_DIR,\n","                     project_name=\"my_first_tuner\"\n","                     )"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"p39oHKAl2aXK"},"source":["tuner_.search(train_x,\n","             train_y, \n","             epochs = 500,\n","             # verbose = 0 (silent) \n","             verbose=0,\n","             batch_size=32,\n","             validation_data = (test_x, test_y),\n","             callbacks = [ClearTrainingOutput()])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"1CNxpcdg3phN"},"source":["tuner_.results_summary()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"IPU-lmv_3AHD"},"source":["tuner_.get_best_hyperparameters()[0].values"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"3TUz0_MtWf9e"},"source":["tuner_.search_space_summary()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"NmAuZCk8gAbP"},"source":["## 2.4 Retrain the model with the optimal hyperparameters"]},{"cell_type":"code","metadata":{"id":"pZi_vqSUcX5p"},"source":["# Build the model with the optimal hyperparameters and train it on the data\n","best_hps = tuner_.get_best_hyperparameters()[0]\n","model = tuner_.hypermodel.build(best_hps)\n","model.fit(train_x, train_y, epochs = 500, validation_data = (test_x, test_y),batch_size=32)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"8simDMO93_Cb"},"source":["loss_, acc_ = model.evaluate(x=test_x,y=test_y, batch_size=32)\n","print('Test loss: %.3f - acc: %.3f' % (loss_, acc_))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"TyDo-xOti6_4"},"source":["model.summary()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"m25iq5vkgpg0"},"source":["<mark>Exercise</mark>\n","\n","Hyperparameter tuning is a time-consuming task. The previous result was not so good. You can try to improve it the tuning considering:\n","- Other [Tuners](https://keras-team.github.io/keras-tuner/documentation/tuners/): BayesianOptimization, Hyperband\n","- Evaluate **max_trials** ranges over 100 or more?\n","- **executions_per_trial** values in [2,3]?\n","- How about you write an article on Medium about Keras Tuner?"]},{"cell_type":"code","metadata":{"id":"4BOmmZWxiInr"},"source":["# PUT YOUR CODE HERE"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"5QL9wGBN363h"},"source":["# 3.0 References"]},{"cell_type":"markdown","metadata":{"id":"2Hfqjn0sXQVh"},"source":["1. https://www.kaggle.com/fchollet/moa-keras-kerastuner-best-practices/\n","2. https://www.kaggle.com/fchollet/titanic-keras-kerastuner-best-practices\n","3. https://www.kaggle.com/fchollet/keras-kerastuner-best-practices\n","4. https://pythonprogramming.net/keras-tuner-optimizing-neural-network-tutorial/\n","5. https://github.com/keras-team/keras-tuner\n","6. https://machinelearningmastery.com/autokeras-for-classification-and-regression/"]}]}