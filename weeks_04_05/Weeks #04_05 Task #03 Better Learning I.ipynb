{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Lesson #03_04 Task #03 Better Learning I.ipynb","provenance":[],"collapsed_sections":["93NYw7S5GyK8","Otpv6Hr1svIK","lUKrWlPKIlix","YmYUA0SPIYMj","0zCigMmEKLow","5w8vjYBRUm7q","zo_tVP7gLL6D"]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"93NYw7S5GyK8"},"source":["# 1 - Introduction"]},{"cell_type":"markdown","metadata":{"id":"2BfNcTCbHbgn"},"source":["Training your neural network requires specifying an initial value of the weights. A well chosen initialization method will help learning.  \n","\n","In the first lessons you probably followed our instructions for weight initialization, and it has worked out so far. But how do you choose the initialization for a new neural network? In this notebook, you will see how different initializations lead to different results. "]},{"cell_type":"markdown","metadata":{"id":"Otpv6Hr1svIK"},"source":["# 2 - Import packages"]},{"cell_type":"code","metadata":{"id":"VJ5o8smyChPk"},"source":["!pip install mlxtend==0.17.3"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"88qrGJpfsx9X"},"source":["import tensorflow as tf\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import datetime\n","import pytz\n","import sklearn\n","import sklearn.datasets\n","import scipy.io\n","import os\n","import time\n","from mlxtend.plotting import plot_decision_regions\n","\n","\n","%matplotlib inline\n","plt.rcParams['figure.figsize'] = (7.0, 4.0) # set default size of plots\n","plt.rcParams['image.interpolation'] = 'nearest'\n","plt.rcParams['image.cmap'] = 'gray'"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"lUKrWlPKIlix"},"source":["# 3 - Import the dataset"]},{"cell_type":"code","metadata":{"id":"PcyWYWiXHzsz"},"source":["def load_dataset():\n","    np.random.seed(1)\n","    train_X, train_Y = sklearn.datasets.make_circles(n_samples=300, noise=.05)\n","    np.random.seed(2)\n","    test_X, test_Y = sklearn.datasets.make_circles(n_samples=100, noise=.05)\n","    # Visualize the data\n","    plt.scatter(train_X[:, 0], train_X[:, 1], c=train_Y.ravel(), s=40, cmap=plt.cm.Spectral);\n","    train_X = train_X\n","    train_Y = train_Y.reshape((train_Y.shape[0],1))\n","    test_X = test_X\n","    test_Y = test_Y.reshape((test_Y.shape[0]),1)\n","    return train_X, train_Y, test_X, test_Y"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"iKIdHkqsITwJ"},"source":["train_x,train_y,test_x,test_y = load_dataset()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"IpZthZNzF8y2"},"source":["print(\"Shape\\nTrain_x: {0} \\t Train_y: {1}\\nTest_x: {2} \\t Test_y: {3}\\n\".format(train_x.shape,train_y.shape,test_x.shape,test_y.shape))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"YmYUA0SPIYMj"},"source":["# 4 - Zero Initialization"]},{"cell_type":"code","metadata":{"id":"Qv0rrwFTIyEK"},"source":["class MyCustomCallback(tf.keras.callbacks.Callback):\n","\n","  def on_train_begin(self, batch, logs=None):\n","    self.begins = time.time()\n","    print('Training: begins at {}'.format(datetime.datetime.now(pytz.timezone('America/Fortaleza')).strftime(\"%a, %d %b %Y %H:%M:%S\")))\n","\n","  def on_train_end(self, logs=None):\n","    print('Training: ends at {}'.format(datetime.datetime.now(pytz.timezone('America/Fortaleza')).strftime(\"%a, %d %b %Y %H:%M:%S\")))\n","    print('Duration: {:.2f} seconds'.format(time.time() - self.begins))    "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"KwbFyl6eI3ah"},"source":["# Configure the random see for the reproducibility purposes\n","tf.random.set_seed(3)\n","\n","# Instantiate a simple classification model\n","model = tf.keras.Sequential([\n","                             tf.keras.layers.Dense(10, activation=tf.nn.relu,kernel_initializer=tf.keras.initializers.Zeros()),\n","                             tf.keras.layers.Dense(5, activation=tf.nn.relu,kernel_initializer=tf.keras.initializers.Zeros()),\n","                             tf.keras.layers.Dense(1, activation = tf.nn.sigmoid,kernel_initializer=tf.keras.initializers.Zeros())\n","                             ])\n","\n","# Instantiate a logistic loss function that expects integer targets (binary classification 0 or 1)\n","loss = tf.keras.losses.BinaryCrossentropy()\n","\n","# Instantiate an accuracy metric.\n","accuracy = tf.keras.metrics.BinaryAccuracy()\n","\n","# Instantiate an optimizer.\n","optimizer = tf.keras.optimizers.SGD(learning_rate=0.01)\n","\n","# configure the optimizer, loss, and metrics to monitor.\n","model.compile(optimizer=optimizer, loss=loss, metrics=[accuracy])\n","\n","# training \n","history = model.fit(x=train_x,\n","                    y=train_y,\n","                    batch_size=32,\n","                    epochs=1000,\n","                    validation_data=(test_x,test_y),\n","                    callbacks=[MyCustomCallback()],\n","                    verbose=0)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"8ZiCzwvPJNIt"},"source":["# summarize history for accuracy\n","plt.plot(history.history['binary_accuracy'])\n","plt.plot(history.history['val_binary_accuracy'])\n","plt.title('model accuracy')\n","plt.ylabel('accuracy')\n","plt.xlabel('epoch')\n","plt.legend(['train', 'test'], loc='best')\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"hNKKUiO_JdkD"},"source":["# summarize history for loss\n","plt.plot(history.history['loss'])\n","plt.plot(history.history['val_loss'])\n","plt.title('model loss')\n","plt.ylabel('loss')\n","plt.xlabel('epoch')\n","plt.legend(['train', 'test'], loc='best')\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"4jk9-fO2Jf7j"},"source":["loss, acc = model.evaluate(x=train_x,y=train_y, batch_size=32)\n","print('Train loss: %.3f - acc: %.3f' % (loss, acc))\n","\n","loss_, acc_ = model.evaluate(x=test_x,y=test_y, batch_size=32)\n","print('Test loss: %.3f - acc: %.3f' % (loss_, acc_))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"TYwpvvcHQRII"},"source":["from mlxtend.plotting import plot_decision_regions\n","# Plot decision boundary\n","plot_decision_regions(test_x,test_y.squeeze(), clf=model,zoom_factor=2.0)\n","plt.title(\"Model with Zeros initialization\")\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"0zCigMmEKLow"},"source":["# 5 - Random Initialization"]},{"cell_type":"code","metadata":{"id":"MvYtmSMeL4CM"},"source":["# Configure the random see for the reproducibility purposes\n","tf.random.set_seed(3)\n","\n","\n","# Instantiate a simple classification model\n","model = tf.keras.Sequential([\n","                             tf.keras.layers.Dense(10, activation=tf.nn.relu,kernel_initializer=tf.keras.initializers.RandomNormal(mean=10,stddev=1)),\n","                             tf.keras.layers.Dense(5, activation=tf.nn.relu,kernel_initializer=tf.keras.initializers.RandomNormal(mean=10,stddev=1)),\n","                             tf.keras.layers.Dense(1, activation = tf.nn.sigmoid,kernel_initializer=tf.keras.initializers.RandomNormal(mean=10,stddev=1))\n","                             ])\n","\n","# Instantiate a logistic loss function that expects integer targets (binary classification 0 or 1)\n","loss = tf.keras.losses.BinaryCrossentropy()\n","\n","# Instantiate an accuracy metric.\n","accuracy = tf.keras.metrics.BinaryAccuracy()\n","\n","# Instantiate an optimizer.\n","optimizer = tf.keras.optimizers.SGD(learning_rate=0.01)\n","\n","# configure the optimizer, loss, and metrics to monitor.\n","model.compile(optimizer=optimizer, loss=loss, metrics=[accuracy])\n","\n","# training \n","history = model.fit(x=train_x,\n","                    y=train_y,\n","                    batch_size=32,\n","                    epochs=1000,\n","                    validation_data=(test_x,test_y),\n","                    callbacks=[MyCustomCallback()],\n","                    verbose=0)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"lDPeHEaGReuq"},"source":["# summarize history for accuracy\n","plt.plot(history.history['binary_accuracy'])\n","plt.plot(history.history['val_binary_accuracy'])\n","plt.title('model accuracy')\n","plt.ylabel('accuracy')\n","plt.xlabel('epoch')\n","plt.legend(['train', 'test'], loc='best')\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"RwHUNZIcReuu"},"source":["# summarize history for loss\n","plt.plot(history.history['loss'])\n","plt.plot(history.history['val_loss'])\n","plt.title('model loss')\n","plt.ylabel('loss')\n","plt.xlabel('epoch')\n","plt.legend(['train', 'test'], loc='best')\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"zgbXn2DdReuw"},"source":["loss, acc = model.evaluate(x=train_x,y=train_y, batch_size=32)\n","print('Train loss: %.3f - acc: %.3f' % (loss, acc))\n","\n","loss_, acc_ = model.evaluate(x=test_x,y=test_y, batch_size=32)\n","print('Test loss: %.3f - acc: %.3f' % (loss_, acc_))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"PUo7Gv1FReuy"},"source":["from mlxtend.plotting import plot_decision_regions\n","# Plot decision boundary\n","plot_decision_regions(test_x,test_y.squeeze(), clf=model,zoom_factor=2.0)\n","plt.title(\"Model with large random initialization\")\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"5w8vjYBRUm7q"},"source":["# 6 - Data Scaling"]},{"cell_type":"code","metadata":{"id":"MK8kbZtnG-H1"},"source":["# Load the TensorBoard notebook extension\n","%load_ext tensorboard"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"I1Ok676VHY2D"},"source":["# Clear any logs from previous runs\n","!rm -rf logs"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"HigImQDnG1LL"},"source":["# mlp with unscaled data for the regression problem\n","from sklearn.datasets import make_regression\n","from tensorflow.keras.layers import Dense\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.optimizers import SGD\n","import matplotlib.pyplot as plt\n","import os\n","\n","# generate regression dataset\n","x, y = make_regression(n_samples=1000, n_features=20, noise=0.1, random_state=1)\n","\n","# split into train and test\n","n_train = 500\n","train_x, test_x = x[:n_train, :], x[n_train:, :]\n","train_y, test_y = y[:n_train], y[n_train:]\n","\n","# define model\n","model = Sequential()\n","model.add(Dense(25, input_dim=20, activation='relu', \n","                kernel_initializer='he_uniform'))\n","model.add(Dense(1, activation='linear'))\n","\n","# compile model\n","model.compile(loss='mean_squared_error', \n","              optimizer=SGD(lr=0.01, momentum=0.9))\n","\n","# callbacks tensorboard\n","logdir = os.path.join(\"logs\", datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\"))\n","tensorboard_callback = tf.keras.callbacks.TensorBoard(logdir, histogram_freq=10)\n","\n","# fit model\n","history = model.fit(train_x, train_y, \n","                    validation_data=(test_x, test_y), \n","                    epochs=100, verbose=0,\n","                    callbacks=[MyCustomCallback(),tensorboard_callback])\n","\n","# evaluate the model\n","train_mse = model.evaluate(train_x, train_y, verbose=0)\n","test_mse = model.evaluate(test_x, test_y, verbose=0)\n","print('Train: %.3f, Test: %.3f' % (train_mse, test_mse))\n","\n","# plot loss during training\n","plt.title('Mean Squared Error')\n","plt.plot(history.history['loss'], label='train')\n","plt.plot(history.history['val_loss'], label='test')\n","plt.legend()\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"m0N8vh5NKxic"},"source":["# mlp with scaled outputs on the regression problem\n","from sklearn.datasets import make_regression\n","from sklearn.preprocessing import StandardScaler\n","from tensorflow.keras.layers import Dense\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.optimizers import SGD\n","import matplotlib.pyplot as plt\n","\n","# generate regression dataset\n","x, y = make_regression(n_samples=1000, n_features=20, noise=0.1, random_state=1)\n","\n","# split into train and test\n","n_train = 500\n","train_x, test_x = x[:n_train, :], x[n_train:, :]\n","train_y, test_y = y[:n_train], y[n_train:]\n","\n","# reshape 1d arrays to 2d arrays\n","train_y = train_y.reshape(len(train_y), 1)\n","test_y = test_y.reshape(len(train_y), 1)\n","\n","# created scaler\n","scaler = StandardScaler()\n","\n","# fit scaler on training dataset\n","scaler.fit(train_y)\n","\n","# transform training dataset\n","train_y = scaler.transform(train_y)\n","\n","# transform test dataset\n","test_y = scaler.transform(test_y)\n","\n","# define model\n","model = Sequential()\n","model.add(Dense(25, input_dim=20, \n","                activation='relu', kernel_initializer='he_uniform'))\n","model.add(Dense(1, activation='linear'))\n","\n","# compile model\n","model.compile(loss='mean_squared_error', \n","              optimizer=SGD(lr=0.01, momentum=0.9))\n","\n","# callbacks tensorboard\n","logdir = os.path.join(\"logs\", datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\"))\n","tensorboard_callback = tf.keras.callbacks.TensorBoard(logdir, histogram_freq=10)\n","\n","\n","# fit model\n","history = model.fit(train_x, train_y, \n","                    validation_data=(test_x, test_y),\n","                    epochs=100, verbose=0,\n","                    callbacks=[MyCustomCallback(),tensorboard_callback])\n","\n","# evaluate the model\n","train_mse = model.evaluate(train_x, train_y, verbose=0)\n","test_mse = model.evaluate(test_x, test_y, verbose=0)\n","print('Train: %.3f, Test: %.3f' % (train_mse, test_mse))\n","\n","# plot loss during training\n","plt.title('Mean Squared Error Loss')\n","plt.plot(history.history['loss'], label='train')\n","plt.plot(history.history['val_loss'], label='test')\n","plt.legend()\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"v1syOw-8Hc3P"},"source":["# Start TensorBoard within the notebook using magics\n","%tensorboard --logdir logs"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Uwt98bn1H4MG"},"source":["fig, ax = plt.subplots(3,1,figsize=(10,6))\n","# histograms of input variables\n","\n","ax[0].hist(train_x[:, 0])\n","ax[0].set_ylabel(\"Feature 0\")\n","ax[0].set_xlim(-4,4)\n","ax[1].hist(train_x[:, 1])\n","ax[1].set_ylabel(\"Feature 1\")\n","ax[1].set_xlim(-4,4)\n","\n","# histogram of target variable\n","ax[2].hist(train_y)\n","ax[2].set_ylabel(\"Output\")\n","ax[2].set_xlim(-4,4)\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"zo_tVP7gLL6D"},"source":["# 7 Fix Vanishing Gradients with Relu"]},{"cell_type":"code","metadata":{"id":"FL8EEaU501V8"},"source":["# mlp with tanh for the two circles classification problem\n","from sklearn.datasets import make_circles\n","from sklearn.preprocessing import MinMaxScaler\n","from tensorflow.keras.layers import Dense\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.optimizers import SGD\n","from tensorflow.keras.initializers import RandomUniform\n","import matplotlib.pyplot as plt\n","\n","# generate 2d classification dataset\n","x, y = make_circles(n_samples=1000, noise=0.1, random_state=1)\n","\n","# scale input data to [-1,1]\n","scaler = MinMaxScaler(feature_range=(-1, 1))\n","x = scaler.fit_transform(x)\n","\n","# split into train and test\n","n_train = 500\n","train_x, test_x = x[:n_train, :], x[n_train:, :]\n","train_y, test_y = y[:n_train], y[n_train:]\n","\n","# define model\n","model = Sequential()\n","\n","# It was also good practice to initialize the network weights\n","# to small random values from a uniform distribution.\n","init = RandomUniform(minval=0, maxval=1)\n","model.add(Dense(5, input_dim=2, activation='tanh', kernel_initializer=init))\n","model.add(Dense(1, activation='sigmoid', kernel_initializer=init))\n","\n","# compile model\n","opt = SGD(lr=0.01, momentum=0.9)\n","model.compile(loss='binary_crossentropy', optimizer=opt, metrics=['accuracy'])\n","\n","# callbacks tensorboard\n","logdir = os.path.join(\"logs\", datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\"))\n","tensorboard_callback = tf.keras.callbacks.TensorBoard(logdir, histogram_freq=1)\n","\n","\n","# fit model\n","history = model.fit(train_x, train_y,\n","                    validation_data=(test_x, test_y),\n","                    epochs=500, verbose=0,\n","                    callbacks=[MyCustomCallback(),tensorboard_callback])\n","\n","# evaluate the model\n","_, train_acc = model.evaluate(train_x, train_y, verbose=0)\n","_, test_acc = model.evaluate(test_x, test_y, verbose=0)\n","print('Train: %.3f, Test: %.3f' % (train_acc, test_acc))\n","\n","# plot loss learning curves\n","plt.subplot(211)\n","plt.title('Cross-Entropy Loss', pad=-40)\n","plt.plot(history.history['loss'], label='train')\n","plt.plot(history.history['val_loss'], label='test')\n","plt.legend()\n","\n","# plot accuracy learning curves\n","plt.subplot(212)\n","plt.title('Accuracy', pad=-40)\n","plt.plot(history.history['accuracy'], label='train')\n","plt.plot(history.history['val_accuracy'], label='test')\n","plt.legend()\n","plt.tight_layout()\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Bnwp3uoy1xa9"},"source":["# Load the TensorBoard notebook extension\n","%load_ext tensorboard"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"EWe_F_lG1xa9"},"source":["# Clear any logs from previous runs if is the case\n","!rm -rf logs"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"xpswYsAU1pU0"},"source":["# Start TensorBoard within the notebook using magics\n","%tensorboard --logdir logs"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"qxBor3H6O_5H"},"source":["# deeper mlp with tanh for the two circles classification problem\n","from sklearn.datasets import make_circles\n","from sklearn.preprocessing import MinMaxScaler\n","from tensorflow.keras.layers import Dense\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.optimizers import SGD\n","from tensorflow.keras.initializers import RandomUniform\n","import matplotlib.pyplot as plt\n","\n","# generate 2d classification dataset\n","x, y = make_circles(n_samples=1000, noise=0.1, random_state=1)\n","scaler = MinMaxScaler(feature_range=(-1, 1))\n","x = scaler.fit_transform(x)\n","\n","# split into train and test\n","n_train = 500\n","train_x, test_x = x[:n_train, :], x[n_train:, :]\n","train_y, test_y = y[:n_train], y[n_train:]\n","\n","# define model\n","init = RandomUniform(minval=0, maxval=1)\n","model = Sequential()\n","model.add(Dense(5, input_dim=2, activation='tanh', kernel_initializer=init))\n","model.add(Dense(5, activation='tanh', kernel_initializer=init))\n","model.add(Dense(5, activation='tanh', kernel_initializer=init))\n","model.add(Dense(5, activation='tanh', kernel_initializer=init))\n","model.add(Dense(5, activation='tanh', kernel_initializer=init))\n","model.add(Dense(1, activation='sigmoid', kernel_initializer=init))\n","\n","# compile model\n","opt = SGD(lr=0.01, momentum=0.9)\n","model.compile(loss='binary_crossentropy', optimizer=opt, metrics=['accuracy'])\n","\n","# callbacks tensorboard\n","logdir = os.path.join(\"logs\", datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\"))\n","tensorboard_callback = tf.keras.callbacks.TensorBoard(logdir, histogram_freq=1)\n","\n","\n","# fit model\n","history = model.fit(train_x, train_y,\n","                    validation_data=(test_x, test_y), epochs=500, verbose=0,\n","                    callbacks=[MyCustomCallback(),tensorboard_callback])\n","\n","# evaluate the model\n","_, train_acc = model.evaluate(train_x, train_y, verbose=0)\n","_, test_acc = model.evaluate(test_x, test_y, verbose=0)\n","print('Train: %.3f, Test: %.3f' % (train_acc, test_acc))\n","\n","# plot loss learning curves\n","plt.subplot(211)\n","plt.title('Cross-Entropy Loss', pad=-40)\n","plt.plot(history.history['loss'], label='train')\n","plt.plot(history.history['val_loss'], label='test')\n","plt.legend()\n","\n","# plot accuracy learning curves\n","plt.subplot(212)\n","plt.title('Accuracy', pad=-40)\n","plt.plot(history.history['accuracy'], label='train')\n","plt.plot(history.history['val_accuracy'], label='test')\n","plt.legend()\n","plt.tight_layout()\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"jRuDi8wpTunL"},"source":["# Start TensorBoard within the notebook using magics\n","%tensorboard --logdir logs"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"NkknCmKJ9tgM"},"source":["# deeper mlp with tanh for the two circles classification problem\n","from sklearn.datasets import make_circles\n","from sklearn.preprocessing import MinMaxScaler\n","from tensorflow.keras.layers import Dense\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.optimizers import SGD\n","from tensorflow.keras.initializers import RandomUniform\n","import matplotlib.pyplot as plt\n","\n","# generate 2d classification dataset\n","x, y = make_circles(n_samples=1000, noise=0.1, random_state=1)\n","scaler = MinMaxScaler(feature_range=(-1, 1))\n","x = scaler.fit_transform(x)\n","\n","# split into train and test\n","n_train = 500\n","train_x, test_x = x[:n_train, :], x[n_train:, :]\n","train_y, test_y = y[:n_train], y[n_train:]\n","\n","# define model\n","init = RandomUniform(minval=0, maxval=1)\n","model = Sequential()\n","model.add(Dense(5, input_dim=2, activation='relu', kernel_initializer=init))\n","model.add(Dense(5, activation='relu', kernel_initializer=init))\n","model.add(Dense(5, activation='relu', kernel_initializer=init))\n","model.add(Dense(5, activation='relu', kernel_initializer=init))\n","model.add(Dense(5, activation='relu', kernel_initializer=init))\n","model.add(Dense(1, activation='sigmoid', kernel_initializer=init))\n","\n","# compile model\n","opt = SGD(lr=0.01, momentum=0.9)\n","model.compile(loss='binary_crossentropy', optimizer=opt, metrics=['accuracy'])\n","\n","# callbacks tensorboard\n","logdir = os.path.join(\"logs\", datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\"))\n","tensorboard_callback = tf.keras.callbacks.TensorBoard(logdir, histogram_freq=1)\n","\n","\n","# fit model\n","history = model.fit(train_x, train_y,\n","                    validation_data=(test_x, test_y), epochs=500, verbose=0,\n","                    callbacks=[MyCustomCallback(),tensorboard_callback])\n","\n","# evaluate the model\n","_, train_acc = model.evaluate(train_x, train_y, verbose=0)\n","_, test_acc = model.evaluate(test_x, test_y, verbose=0)\n","print('Train: %.3f, Test: %.3f' % (train_acc, test_acc))\n","\n","# plot loss learning curves\n","plt.subplot(211)\n","plt.title('Cross-Entropy Loss', pad=-40)\n","plt.plot(history.history['loss'], label='train')\n","plt.plot(history.history['val_loss'], label='test')\n","plt.legend()\n","\n","# plot accuracy learning curves\n","plt.subplot(212)\n","plt.title('Accuracy', pad=-40)\n","plt.plot(history.history['accuracy'], label='train')\n","plt.plot(history.history['val_accuracy'], label='test')\n","plt.legend()\n","plt.tight_layout()\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"TlEL6VwE_ruH"},"source":["# Start TensorBoard within the notebook using magics\n","%tensorboard --logdir logs"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ebM62LrLAgg4"},"source":["# deeper mlp with relu for the two circles classification problem (5 hidden layers)\n","from sklearn.datasets import make_circles\n","from sklearn.preprocessing import MinMaxScaler\n","from tensorflow.keras.layers import Dense\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.optimizers import SGD\n","import matplotlib.pyplot as plt\n","\n","# generate 2d classification dataset\n","x, y = make_circles(n_samples=1000, noise=0.1, random_state=1)\n","scaler = MinMaxScaler(feature_range=(-1, 1))\n","x = scaler.fit_transform(x)\n","\n","# split into train and test\n","n_train = 500\n","train_x, test_x = x[:n_train, :], x[n_train:, :]\n","train_y, test_y = y[:n_train], y[n_train:]\n","\n","# define model\n","model = Sequential()\n","model.add(Dense(5, input_dim=2, activation='relu', kernel_initializer='he_uniform'))\n","model.add(Dense(5, activation='relu', kernel_initializer='he_uniform'))\n","model.add(Dense(5, activation='relu', kernel_initializer='he_uniform'))\n","model.add(Dense(5, activation='relu', kernel_initializer='he_uniform'))\n","model.add(Dense(5, activation='relu', kernel_initializer='he_uniform'))\n","model.add(Dense(1, activation='sigmoid'))\n","\n","# compile model\n","opt = SGD(lr=0.01, momentum=0.9)\n","model.compile(loss='binary_crossentropy', optimizer=opt, metrics=['accuracy'])\n","\n","# callbacks tensorboard\n","logdir = os.path.join(\"logs\", datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\"))\n","tensorboard_callback = tf.keras.callbacks.TensorBoard(logdir, histogram_freq=1)\n","\n","\n","# fit model\n","history = model.fit(train_x, train_y,\n","                    validation_data=(test_x, test_y),\n","                    epochs=500, verbose=0,\n","                    callbacks=[MyCustomCallback(),tensorboard_callback])\n","\n","# evaluate the model\n","_, train_acc = model.evaluate(train_x, train_y, verbose=0)\n","_, test_acc = model.evaluate(test_x, test_y, verbose=0)\n","print('Train: %.3f, Test: %.3f' % (train_acc, test_acc))\n","\n","# plot loss learning curves\n","plt.subplot(211)\n","plt.title('Cross-Entropy Loss', pad=-40)\n","plt.plot(history.history['loss'], label='train')\n","plt.plot(history.history['val_loss'], label='test')\n","plt.legend()\n","\n","# plot accuracy learning curves\n","plt.subplot(212)\n","plt.title('Accuracy', pad=-40)\n","plt.plot(history.history['accuracy'], label='train')\n","plt.plot(history.history['val_accuracy'], label='test')\n","plt.legend()\n","plt.tight_layout()\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"1YIRn3BHCidK"},"source":["# Start TensorBoard within the notebook using magics\n","%tensorboard --logdir logs"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"QADbdO6lDMti"},"source":["# deeper mlp with relu for the two circles classification problem (5 hidden layers)\n","from sklearn.datasets import make_circles\n","from sklearn.preprocessing import MinMaxScaler\n","from tensorflow.keras.layers import Dense\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.optimizers import SGD\n","import matplotlib.pyplot as plt\n","\n","# generate 2d classification dataset\n","x, y = make_circles(n_samples=1000, noise=0.1, random_state=1)\n","scaler = MinMaxScaler(feature_range=(-1, 1))\n","x = scaler.fit_transform(x)\n","\n","# split into train and test\n","n_train = 500\n","train_x, test_x = x[:n_train, :], x[n_train:, :]\n","train_y, test_y = y[:n_train], y[n_train:]\n","\n","# define model\n","model = Sequential()\n","model.add(Dense(5, input_dim=2, activation='tanh', kernel_initializer='he_uniform'))\n","model.add(Dense(5, activation='tanh', kernel_initializer='he_uniform'))\n","model.add(Dense(5, activation='tanh', kernel_initializer='he_uniform'))\n","model.add(Dense(5, activation='tanh', kernel_initializer='he_uniform'))\n","model.add(Dense(5, activation='tanh', kernel_initializer='he_uniform'))\n","model.add(Dense(1, activation='sigmoid'))\n","\n","# compile model\n","opt = SGD(lr=0.01, momentum=0.9)\n","model.compile(loss='binary_crossentropy', optimizer=opt, metrics=['accuracy'])\n","\n","# callbacks tensorboard\n","logdir = os.path.join(\"logs\", datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\"))\n","tensorboard_callback = tf.keras.callbacks.TensorBoard(logdir, histogram_freq=1)\n","\n","\n","# fit model\n","history = model.fit(train_x, train_y,\n","                    validation_data=(test_x, test_y),\n","                    epochs=500, verbose=0,\n","                    callbacks=[MyCustomCallback(),tensorboard_callback])\n","\n","# evaluate the model\n","_, train_acc = model.evaluate(train_x, train_y, verbose=0)\n","_, test_acc = model.evaluate(test_x, test_y, verbose=0)\n","print('Train: %.3f, Test: %.3f' % (train_acc, test_acc))\n","\n","# plot loss learning curves\n","plt.subplot(211)\n","plt.title('Cross-Entropy Loss', pad=-40)\n","plt.plot(history.history['loss'], label='train')\n","plt.plot(history.history['val_loss'], label='test')\n","plt.legend()\n","\n","# plot accuracy learning curves\n","plt.subplot(212)\n","plt.title('Accuracy', pad=-40)\n","plt.plot(history.history['accuracy'], label='train')\n","plt.plot(history.history['val_accuracy'], label='test')\n","plt.legend()\n","plt.tight_layout()\n","plt.show()"],"execution_count":null,"outputs":[]}]}