{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Lesson #03_04 Task #01 Better Generalization I.ipynb","provenance":[],"collapsed_sections":["-CE5LY7lu0WS","MMxX_wG4lbGU","uB3_YIlEx1Do","tkf7WAAbx1EI","q2cF42K2FPD3"]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"-CE5LY7lu0WS"},"source":["# 1 - Introduction"]},{"cell_type":"markdown","metadata":{"id":"C-EGMZ_68Hk9"},"source":["There is not a lot of code required, but we are going to step over it slowly so that you will know how to create your own models in the future. The steps you are going to cover in this practical assignment are as follows:\n","\n","1. Load Data\n","2. Define Model\n","3. Compile Model\n","4. Fit Model\n","5. Evaluate Model\n","6. Tie It All Together\n","7. Make Predictions"]},{"cell_type":"markdown","metadata":{"id":"Otpv6Hr1svIK"},"source":["## Import packages"]},{"cell_type":"code","metadata":{"id":"VJ5o8smyChPk"},"source":["!pip install mlxtend==0.17.3"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Nf8JEmDoXgMq"},"source":["# Load the TensorBoard notebook extension\n","%load_ext tensorboard"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"85HpkSMAhJWa"},"source":["# Clear any logs from previous runs\n","!rm -rf logs"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"88qrGJpfsx9X"},"source":["import tensorflow as tf\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import sklearn\n","import sklearn.datasets\n","import scipy.io\n","import time\n","from time import gmtime, strftime\n","import datetime\n","import os\n","import pytz\n","from mlxtend.plotting import plot_decision_regions\n","\n","%matplotlib inline\n","plt.rcParams['figure.figsize'] = (7.0, 4.0) # set default size of plots\n","plt.rcParams['image.interpolation'] = 'nearest'\n","plt.rcParams['image.cmap'] = 'gray'"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"TyO9yST1Fl_M"},"source":["tf.__version__"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"MMxX_wG4lbGU"},"source":["# 2 - Overfit multilayer perceptron"]},{"cell_type":"code","metadata":{"id":"FsEw115Yl9Gq"},"source":["class MyCustomCallback(tf.keras.callbacks.Callback):\n","\n","  def on_train_begin(self, batch, logs=None):\n","    self.begins = time.time()\n","    print('Training: begins at {}'.format(datetime.datetime.now(pytz.timezone('America/Fortaleza')).strftime(\"%a, %d %b %Y %H:%M:%S\")))\n","\n","  def on_train_end(self, logs=None):\n","    print('Training: ends at {}'.format(datetime.datetime.now(pytz.timezone('America/Fortaleza')).strftime(\"%a, %d %b %Y %H:%M:%S\")))\n","    print('Duration: {:.2f} seconds'.format(time.time() - self.begins))    "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"-plq4rJxlZam"},"source":["# overfit mlp for the moons dataset\n","from sklearn.datasets import make_moons\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Dense\n","import matplotlib.pyplot as plt\n","\n","# generate 2d classification dataset\n","x, y = make_moons(n_samples=100, noise=0.2, random_state=1)\n","\n","# split into train and test sets\n","n_train = 30\n","train_x, test_x = x[:n_train, :], x[n_train:, :]\n","train_y, test_y = y[:n_train], y[n_train:]\n","\n","# define model\n","model = Sequential()\n","model.add(Dense(500, input_dim=2, activation='relu'))\n","model.add(Dense(1, activation='sigmoid'))\n","model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n","\n","# callbacks tensorboard\n","logdir = os.path.join(\"logs\", datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\"))\n","tensorboard_callback = tf.keras.callbacks.TensorBoard(logdir, histogram_freq=100)\n","\n","# fit model\n","history = model.fit(train_x, train_y,\n","                    validation_data=(test_x, test_y), \n","                    epochs=4000, verbose=0,batch_size=32,\n","                    callbacks=[MyCustomCallback(),tensorboard_callback])\n","\n","# evaluate the model\n","_, train_acc = model.evaluate(train_x, train_y, verbose=0)\n","_, test_acc = model.evaluate(test_x, test_y, verbose=0)\n","print('Train: %.3f, Test: %.3f' % (train_acc, test_acc))\n","\n","# plot loss learning curves\n","plt.subplot(211)\n","plt.title('Cross-Entropy Loss', pad=-40)\n","plt.plot(history.history['loss'], label='train')\n","plt.plot(history.history['val_loss'], label='test')\n","plt.legend()\n","\n","# plot accuracy learning curves\n","plt.subplot(212)\n","plt.title('Accuracy', pad=-40)\n","plt.plot(history.history['accuracy'], label='train')\n","plt.plot(history.history['val_accuracy'], label='test')\n","plt.legend()\n","plt.tight_layout()\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"h2vPSeVpm8ua"},"source":["from mlxtend.plotting import plot_decision_regions\n","# Plot decision boundary\n","plot_decision_regions(test_x,test_y.squeeze(), clf=model,zoom_factor=2.0)\n","plt.title(\"Model without regularization\")\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Nto21ovNn4bk"},"source":["# Start TensorBoard within the notebook using magics\n","%tensorboard --logdir logs"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"uB3_YIlEx1Do"},"source":["# 3 - L2 Regularization\n"]},{"cell_type":"markdown","metadata":{"id":"4l02mx3Q8251"},"source":["\n","The standard way to avoid overfitting is called **L2 regularization**. It consists of appropriately modifying your cost function, from:\n","$$J = -\\frac{1}{m} \\sum\\limits_{i = 1}^{m} \\left( \\small  y^{(i)}\\log\\left(\\hat{y}^{(i)}\\right) + (1-y^{(i)})\\log\\left(1- \\hat{y}^{(i)}\\right) \\right) \\tag{1}$$\n","\n","\n","To:\n","$$J_{regularized} = \\small \\underbrace{-\\frac{1}{m} \\sum\\limits_{i = 1}^{m} \\large{(}\\small y^{(i)}\\log\\left(\\hat{y}^{(i)}\\right) + (1-y^{(i)})\\log\\left(1- \\hat{y}^{(i)}\\right) \\large{)} }_\\text{cross-entropy cost} + \\underbrace{\\frac{1}{m} \\frac{\\lambda}{2} \\sum\\limits_l\\sum\\limits_k\\sum\\limits_j W_{k,j}^{[l]2} }_\\text{L2 regularization cost} \\tag{2}$$\n"]},{"cell_type":"code","metadata":{"id":"qfKzscuQqGvK"},"source":["# mlp with weight regularization for the moons dataset\n","from sklearn.datasets import make_moons\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Dense\n","from tensorflow.keras.regularizers import l2\n","import matplotlib.pyplot as plt\n","\n","# generate 2d classification dataset\n","x, y = make_moons(n_samples=100, noise=0.2, random_state=1)\n","\n","# split into train and test sets\n","n_train = 30\n","train_x, test_x = x[:n_train, :], x[n_train:, :]\n","train_y, test_y = y[:n_train], y[n_train:]\n","\n","# define model\n","model_l2 = Sequential()\n","model_l2.add(Dense(500, input_dim=2, activation='relu',\n","                kernel_regularizer=l2(0.001)))\n","model_l2.add(Dense(1, activation='sigmoid'))\n","model_l2.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n","\n","# callbacks tensorboard\n","logdir = os.path.join(\"logs\", datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\"))\n","tensorboard_callback = tf.keras.callbacks.TensorBoard(logdir, histogram_freq=100)\n","\n","# fit model\n","history_l2 = model_l2.fit(train_x, train_y, \n","                    validation_data=(test_x, test_y),\n","                    epochs=4000, verbose=0,\n","                    callbacks=[MyCustomCallback(),tensorboard_callback])\n","\n","# evaluate the model\n","_, train_acc = model_l2.evaluate(train_x, train_y, verbose=0)\n","_, test_acc = model_l2.evaluate(test_x, test_y, verbose=0)\n","print('Train: %.3f, Test: %.3f' % (train_acc, test_acc))\n","\n","# plot loss learning curves\n","plt.subplot(211)\n","plt.title('Cross-Entropy Loss', pad=-40)\n","plt.plot(history_l2.history['loss'], label='train')\n","plt.plot(history_l2.history['val_loss'], label='test')\n","plt.legend()\n","\n","# plot accuracy learning curves\n","plt.subplot(212)\n","plt.title('Accuracy', pad=-40)\n","plt.plot(history_l2.history['accuracy'], label='train')\n","plt.plot(history_l2.history['val_accuracy'], label='test')\n","plt.legend()\n","plt.tight_layout()\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"8k4vCne3wxaR"},"source":["from mlxtend.plotting import plot_decision_regions\n","# Plot decision boundary\n","plot_decision_regions(test_x,test_y.squeeze(), clf=model_l2,zoom_factor=2.0)\n","plt.title(\"Model with regularization\")\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ICM4TUHNbYyO"},"source":["# Start TensorBoard within the notebook using magics\n","%tensorboard --logdir logs"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"tkf7WAAbx1EI"},"source":["# 4 - Dropout\n"]},{"cell_type":"markdown","metadata":{"id":"RlCshhJ9-mnl"},"source":["\n","Finally, **dropout** is a widely used regularization technique that is specific to deep learning. \n","**It randomly shuts down some neurons in each iteration.** Watch these two animations to see what this means!\n","\n","<!--\n","To understand drop-out, consider this conversation with a friend:\n","- Friend: \"Why do you need all these neurons to train your network and classify images?\". \n","- You: \"Because each neuron contains a weight and can learn specific features/details/shape of an image. The more neurons I have, the more featurse my model learns!\"\n","- Friend: \"I see, but are you sure that your neurons are learning different features and not all the same features?\"\n","- You: \"Good point... Neurons in the same layer actually don't talk to each other. It should be definitly possible that they learn the same image features/shapes/forms/details... which would be redundant. There should be a solution.\"\n","!--> \n","\n","<center><img width=\"600\" src=\"https://drive.google.com/uc?export=view&id=1eNMeI3JkcXQ3_AeOUItFfgf8D6tSexg0\"></center>\n","<br>\n","<caption><center> <b>Figure 1</b>: Drop-out on the second hidden layer. <br> At each iteration, you shut down (= set to zero) each neuron of a layer with probability $1 - keep\\_prob$ or keep it with probability $keep\\_prob$ (50% here). The dropped neurons don't contribute to the training in both the forward and backward propagations of the iteration. </center></caption>\n","\n","<center><img width=\"600\" src=\"https://drive.google.com/uc?export=view&id=1GfLbHLOQ2xzsDOQZ-d4m0ZmljZeBHAOz\"></center>\n","<caption><center> <b>Figure 2</b>: Drop-out on the first and third hidden layers. <br> $1^{st}$ layer: we shut down on average 40% of the neurons.  $3^{rd}$ layer: we shut down on average 20% of the neurons. </center></caption>\n","\n","\n","When you shut some neurons down, you actually modify your model. The idea behind drop-out is that at each iteration, you train a different model that uses only a subset of your neurons. With dropout, your neurons thus become less sensitive to the activation of one other specific neuron, because that other neuron might be shut down at any time. \n"]},{"cell_type":"code","metadata":{"id":"FpbWii871hzS"},"source":["# mlp with weight regularization for the moons dataset\n","from sklearn.datasets import make_moons\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Dense\n","from tensorflow.keras.layers import Dropout\n","from tensorflow.keras.regularizers import l2\n","import matplotlib.pyplot as plt\n","\n","# generate 2d classification dataset\n","x, y = make_moons(n_samples=100, noise=0.2, random_state=1)\n","\n","# split into train and test sets\n","n_train = 30\n","train_x, test_x = x[:n_train, :], x[n_train:, :]\n","train_y, test_y = y[:n_train], y[n_train:]\n","\n","# define model\n","model_dropout = Sequential()\n","model_dropout.add(Dense(500, input_dim=2, activation='relu'))\n","model_dropout.add(Dropout(0.4))\n","model_dropout.add(Dense(1, activation='sigmoid'))\n","model_dropout.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n","\n","# callbacks tensorboard\n","logdir = os.path.join(\"logs\", datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\"))\n","tensorboard_callback = tf.keras.callbacks.TensorBoard(logdir, histogram_freq=100)\n","\n","# fit model\n","history_dropout = model_dropout.fit(train_x, train_y, \n","                    validation_data=(test_x, test_y),\n","                    epochs=4000, verbose=0,\n","                    callbacks=[MyCustomCallback(),tensorboard_callback])\n","\n","# evaluate the model\n","_, train_acc = model_dropout.evaluate(train_x, train_y, verbose=0)\n","_, test_acc = model_dropout.evaluate(test_x, test_y, verbose=0)\n","print('Train: %.3f, Test: %.3f' % (train_acc, test_acc))\n","\n","# plot loss learning curves\n","plt.subplot(211)\n","plt.title('Cross-Entropy Loss', pad=-40)\n","plt.plot(history_dropout.history['loss'], label='train')\n","plt.plot(history_dropout.history['val_loss'], label='test')\n","plt.legend()\n","\n","# plot accuracy learning curves\n","plt.subplot(212)\n","plt.title('Accuracy', pad=-40)\n","plt.plot(history_dropout.history['accuracy'], label='train')\n","plt.plot(history_dropout.history['val_accuracy'], label='test')\n","plt.legend()\n","plt.tight_layout()\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Z-nFSIaGqU-a"},"source":["from mlxtend.plotting import plot_decision_regions\n","# Plot decision boundary\n","plot_decision_regions(test_x,test_y.squeeze(), clf=model_dropout,zoom_factor=2.0)\n","plt.title(\"Model with dropout\")\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"5eoZoKAa2rnJ"},"source":["# Start TensorBoard within the notebook using magics\n","%tensorboard --logdir logs"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"q2cF42K2FPD3"},"source":["# 5 - L2 vs Dropout"]},{"cell_type":"code","metadata":{"id":"53fGLuKeMFgG"},"source":["def print_analysis(titles,history,loss=True):\n","  if loss:\n","    func = \"loss\"\n","    func_val = \"val_loss\"\n","  else:\n","    func = \"binary_accuracy\"\n","    func_val = \"val_binary_accuracy\"\n","\n","  f, axs = plt.subplots(1,len(titles),figsize=(12,6))\n","   \n","  for i, title in enumerate(titles):\n","    axs[i].set_title(title)\n","    axs[i].plot(history[i].history[func])\n","    axs[i].plot(history[i].history[func_val])\n","    axs[i].set_ylabel(func)\n","    axs[i].set_xlabel('epoch')\n","    axs[i].legend(['train', 'test'], loc='best')\n","  \n","  plt.tight_layout()\n","  plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"i1Q68J32Nrcj"},"source":["titles = ['Model without regularization','Model with regularization L2','Model with dropout']\n","hist = [history,history_l2,history_dropout]\n","print_analysis(titles,hist,loss=True)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"WyCRtKAIPgIY"},"source":["def print_regions(titles,models):\n","\n","  f, axs = plt.subplots(1,len(titles),figsize=(12,4))\n","   \n","  for i, title in enumerate(titles):\n","    plot_decision_regions(test_x,test_y.squeeze(), clf=models[i],zoom_factor=2.0,ax=axs[i])\n","    axs[i].set_title(title)\n","  plt.tight_layout()\n","  plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"668zLQ3BQFNA"},"source":["models = [model,model_l2,model_dropout]\n","print_regions(titles,models)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"3ZnGlZK43WBi"},"source":["# 6 - Force Small Weights with Weight Constraints"]},{"cell_type":"code","metadata":{"id":"loAB8_wRURLy"},"source":["# mlp overfit on the moons dataset with a unit norm constraint\n","from sklearn.datasets import make_moons\n","from tensorflow.keras.layers import Dense\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.constraints import unit_norm\n","import matplotlib.pyplot as plt\n","import os\n","\n","# generate 2d classification dataset\n","x, y = make_moons(n_samples=100, noise=0.2, random_state=1)\n","\n","# split into train and test\n","n_train = 30\n","train_x, test_x = x[:n_train, :], x[n_train:, :]\n","train_y, test_y = y[:n_train], y[n_train:]\n","\n","# define model\n","model = Sequential()\n","model.add(Dense(500, input_dim=2, activation='relu', kernel_constraint=unit_norm()))\n","#kernel_constraint=tf.keras.constraints.min_max_norm(min_value=-0.2, max_value=1.0)))\n","model.add(Dense(1, activation='sigmoid'))\n","model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n","\n","# callbacks tensorboard\n","logdir = os.path.join(\"logs\", datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\"))\n","tensorboard_callback = tf.keras.callbacks.TensorBoard(logdir, histogram_freq=100)\n","\n","# fit model\n","history = model.fit(train_x, train_y,\n","                    validation_data=(test_x, test_y),\n","                    epochs=4000, verbose=0,\n","                    callbacks=[MyCustomCallback(),tensorboard_callback])\n","\n","# evaluate the model\n","_, train_acc = model.evaluate(train_x, train_y, verbose=0)\n","_, test_acc = model.evaluate(test_x, test_y, verbose=0)\n","print('Train: %.3f, Test: %.3f' % (train_acc, test_acc))\n","\n","# plot loss learning curves\n","plt.subplot(211)\n","plt.title('Cross-Entropy Loss', pad=-40)\n","plt.plot(history.history['loss'], label='train')\n","plt.plot(history.history['val_loss'], label='test')\n","plt.legend()\n","\n","# plot accuracy learning curves\n","plt.subplot(212)\n","plt.title('Accuracy', pad=-40)\n","plt.plot(history.history['accuracy'], label='train')\n","plt.plot(history.history['val_accuracy'], label='test')\n","plt.legend()\n","plt.tight_layout()\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"BHChR2tCUrpv"},"source":["from mlxtend.plotting import plot_decision_regions\n","# Plot decision boundary\n","plot_decision_regions(test_x,test_y.squeeze(), clf=model,zoom_factor=2.0)\n","plt.title(\"Model with weights constraints\")\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"PWus4c-GaYYH"},"source":["# Start TensorBoard within the notebook using magics\n","%tensorboard --logdir logs"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"FtdmGRF3iLPB"},"source":["filter = tf.keras.constraints.UnitNorm()\n","data = np.arange(3).reshape(3, 1).astype(np.float32)\n","print(data)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"2l86cAzCib0f"},"source":["filter(data)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"iaLEf9vqiiGQ"},"source":["np.linalg.norm(filter(data))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ngq7qmwWlvxm"},"source":["np.linalg.norm(data)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ymjSWu26l5pC"},"source":["data/np.linalg.norm(data)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"I7_8PhubebIR"},"source":["filter = tf.keras.constraints.UnitNorm()\n","data = np.arange(6).reshape(3, 2).astype(np.float32)\n","data"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"-l_ODs3RevKr"},"source":["filter(data)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"eCFtK1cqo8wI"},"source":["np.linalg.norm(filter(data),axis=0)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Ha4YL7P1e2Kk"},"source":["np.linalg.norm(data,axis=0)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"fTtiuqSBfsBC"},"source":["data/np.linalg.norm(data,axis=0)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"O10e_wAiaxnv"},"source":["# mlp overfit on the moons dataset with a unit norm constraint\n","from sklearn.datasets import make_moons\n","from tensorflow.keras.layers import Dense\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.constraints import unit_norm\n","import matplotlib.pyplot as plt\n","import os\n","\n","# generate 2d classification dataset\n","x, y = make_moons(n_samples=100, noise=0.2, random_state=1)\n","\n","# split into train and test\n","n_train = 30\n","train_x, test_x = x[:n_train, :], x[n_train:, :]\n","train_y, test_y = y[:n_train], y[n_train:]\n","\n","# define model\n","model = Sequential()\n","model.add(Dense(500, input_dim=2, activation='relu', \n","                kernel_constraint=tf.keras.constraints.min_max_norm(min_value=-0.2, max_value=1.0)))\n","model.add(Dense(1, activation='sigmoid'))\n","model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n","\n","# callbacks tensorboard\n","logdir = os.path.join(\"logs\", datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\"))\n","tensorboard_callback = tf.keras.callbacks.TensorBoard(logdir, histogram_freq=100)\n","\n","\n","# fit model\n","history = model.fit(train_x, train_y,\n","                    validation_data=(test_x, test_y),\n","                    epochs=4000, verbose=0,\n","                    callbacks=[MyCustomCallback(),tensorboard_callback])\n","\n","# evaluate the model\n","_, train_acc = model.evaluate(train_x, train_y, verbose=0)\n","_, test_acc = model.evaluate(test_x, test_y, verbose=0)\n","print('Train: %.3f, Test: %.3f' % (train_acc, test_acc))\n","\n","# plot loss learning curves\n","plt.subplot(211)\n","plt.title('Cross-Entropy Loss', pad=-40)\n","plt.plot(history.history['loss'], label='train')\n","plt.plot(history.history['val_loss'], label='test')\n","plt.legend()\n","\n","# plot accuracy learning curves\n","plt.subplot(212)\n","plt.title('Accuracy', pad=-40)\n","plt.plot(history.history['accuracy'], label='train')\n","plt.plot(history.history['val_accuracy'], label='test')\n","plt.legend()\n","plt.tight_layout()\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Hpt_bysOqgoP"},"source":["from mlxtend.plotting import plot_decision_regions\n","# Plot decision boundary\n","plot_decision_regions(test_x,test_y.squeeze(), clf=model,zoom_factor=2.0)\n","plt.title(\"Model with weights constraints\")\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"gYjUEqSarISm"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"XEVcChn-mXAd"},"source":[""],"execution_count":null,"outputs":[]}]}